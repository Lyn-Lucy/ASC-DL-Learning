{"cells":[{"cell_type":"markdown","metadata":{},"source":["# ResNet_Plus"]},{"cell_type":"markdown","metadata":{},"source":["- F(x)=G(x)+Residual\n","- 在已有模型的基础上拓宽函数空间，让模型至少不会训练偏"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-04-16T14:20:25.572474Z","iopub.status.busy":"2022-04-16T14:20:25.572188Z","iopub.status.idle":"2022-04-16T14:20:33.288158Z","shell.execute_reply":"2022-04-16T14:20:33.287323Z","shell.execute_reply.started":"2022-04-16T14:20:25.572426Z"},"trusted":true},"outputs":[],"source":["! pip install d2l"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-04-16T14:20:33.295596Z","iopub.status.busy":"2022-04-16T14:20:33.294797Z","iopub.status.idle":"2022-04-16T14:20:33.884201Z","shell.execute_reply":"2022-04-16T14:20:33.883411Z","shell.execute_reply.started":"2022-04-16T14:20:33.295560Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch import nn\n","import torchvision\n","from torch.nn import functional as F\n","from d2l import torch as d2l\n","import os\n","\n","class Residual(nn.Module):  #@save\n","    def __init__(self, input_channels, num_channels,use_1x1conv=False, strides=1):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(input_channels, num_channels,\n","                               kernel_size=3, padding=1, stride=strides)\n","        self.conv2 = nn.Conv2d(num_channels, num_channels,\n","                               kernel_size=3, padding=1)\n","        if use_1x1conv:\n","            self.conv3 = nn.Conv2d(input_channels, num_channels,\n","                                   kernel_size=1, stride=strides)\n","        else:\n","            self.conv3 = None\n","        self.bn1 = nn.BatchNorm2d(num_channels)\n","        self.bn2 = nn.BatchNorm2d(num_channels)\n","\n","    def forward(self, X):\n","        Y = F.relu(self.bn1(self.conv1(X)))\n","        Y = self.bn2(self.conv2(Y))\n","        if self.conv3:\n","            X = self.conv3(X)\n","        Y += X\n","        return F.relu(Y)"]},{"cell_type":"markdown","metadata":{},"source":["- CABM注意力机制"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-04-16T14:20:33.885857Z","iopub.status.busy":"2022-04-16T14:20:33.885608Z","iopub.status.idle":"2022-04-16T14:20:33.901380Z","shell.execute_reply":"2022-04-16T14:20:33.899768Z","shell.execute_reply.started":"2022-04-16T14:20:33.885824Z"},"trusted":true},"outputs":[],"source":["class CALayer(nn.Module):  # Channel Attention (CA) Layer\n","    def __init__(self, in_channels, reduction=16, pool_types=['avg', 'max']):\n","        super().__init__()\n","        self.pool_list = ['avg', 'max']\n","        self.pool_types = pool_types\n","        self.in_channels = in_channels\n","        self.Pool = [nn.AdaptiveAvgPool2d(1), nn.AdaptiveMaxPool2d(1, return_indices=False)]\n","        self.conv_ca = nn.Sequential(\n","            nn.Conv2d(in_channels, in_channels //\n","                      reduction, 1, padding=0, bias=True),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(in_channels // reduction,\n","                      in_channels, 1, padding=0, bias=True)\n","        )\n","\n","    def forward(self, x):\n","        for (i, pool_type) in enumerate(self.pool_types):\n","            pool = self.Pool[self.pool_list.index(pool_type)](x)\n","            channel_att_raw = self.conv_ca(pool)\n","            if i == 0:\n","                channel_att_sum = channel_att_raw\n","            else:\n","                channel_att_sum += channel_att_raw\n","        scale = F.sigmoid(channel_att_sum)\n","        return x * scale\n","\n","\n","class SALayer(nn.Module):  # Spatial Attention Layer\n","    def __init__(self):\n","        super().__init__()\n","        self.conv_sa = nn.Sequential(\n","            nn.Conv2d(2, 1, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(1, momentum=0.01),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        x_compress = torch.cat(\n","            (torch.max(x, 1, keepdim=True)[0], torch.mean(x, dim=1, keepdim=True)), dim=1)\n","        scale = self.conv_sa(x_compress)\n","        return x * scale\n","\n","\n","class CBAM(nn.Module):\n","    def __init__(self, in_channels, reduction=2, pool_types=['avg', 'max']):\n","        super().__init__()\n","        self.CALayer = CALayer(\n","            in_channels, reduction, pool_types)\n","        self.SALayer = SALayer()\n","\n","    def forward(self, x):\n","        x_out = self.CALayer(x)\n","        x_out = self.SALayer(x_out)\n","        return x_out"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-04-16T14:20:33.904633Z","iopub.status.busy":"2022-04-16T14:20:33.904210Z","iopub.status.idle":"2022-04-16T14:20:33.916609Z","shell.execute_reply":"2022-04-16T14:20:33.915962Z","shell.execute_reply.started":"2022-04-16T14:20:33.904604Z"},"trusted":true},"outputs":[],"source":["b1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n","                   nn.BatchNorm2d(64), nn.ReLU(),\n","                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-04-16T14:20:33.919630Z","iopub.status.busy":"2022-04-16T14:20:33.918853Z","iopub.status.idle":"2022-04-16T14:20:33.928066Z","shell.execute_reply":"2022-04-16T14:20:33.927299Z","shell.execute_reply.started":"2022-04-16T14:20:33.919589Z"},"trusted":true},"outputs":[],"source":["def resnet_block(input_channels, num_channels, num_residuals,\n","                 first_block=False):\n","    blk = []\n","    for i in range(num_residuals):\n","        if i == 0 and not first_block:\n","            blk.append(Residual(input_channels, num_channels,\n","                                use_1x1conv=True, strides=2))\n","        else:\n","            blk.append(Residual(num_channels, num_channels))\n","    return blk"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-04-16T14:20:33.931185Z","iopub.status.busy":"2022-04-16T14:20:33.930968Z","iopub.status.idle":"2022-04-16T14:20:34.039582Z","shell.execute_reply":"2022-04-16T14:20:34.038898Z","shell.execute_reply.started":"2022-04-16T14:20:33.931162Z"},"trusted":true},"outputs":[],"source":["b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\n","b3 = nn.Sequential(*resnet_block(64, 128, 2))\n","b4 = nn.Sequential(*resnet_block(128, 256, 2))\n","b5 = nn.Sequential(*resnet_block(256, 512, 2),CBAM(512))"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-04-16T14:20:34.040947Z","iopub.status.busy":"2022-04-16T14:20:34.040697Z","iopub.status.idle":"2022-04-16T14:20:34.046107Z","shell.execute_reply":"2022-04-16T14:20:34.045170Z","shell.execute_reply.started":"2022-04-16T14:20:34.040901Z"},"trusted":true},"outputs":[],"source":["net = nn.Sequential(b1, b2, b3, b4, b5,\n","                    nn.AdaptiveAvgPool2d((1,1)),\n","                    nn.Flatten(),nn.Linear(512, 10))"]},{"cell_type":"markdown","metadata":{},"source":["- Data augumentation"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-04-16T14:20:34.048221Z","iopub.status.busy":"2022-04-16T14:20:34.047648Z","iopub.status.idle":"2022-04-16T14:20:34.056636Z","shell.execute_reply":"2022-04-16T14:20:34.055975Z","shell.execute_reply.started":"2022-04-16T14:20:34.048185Z"},"trusted":true},"outputs":[],"source":["# cifar10 32*32\n","train_augs = torchvision.transforms.Compose([\n","     # 在高度和宽度上将图像放大到40像素的正方形\n","    torchvision.transforms.Resize(40),\n","    # 随机裁剪出一个高度和宽度均为40像素的正方形图像，\n","    # 生成一个面积为原始图像面积0.64到1倍的小正方形，\n","    # 然后将其缩放为高度和宽度均为32像素的正方形\n","    torchvision.transforms.RandomResizedCrop(32, scale=(0.64, 1.0),ratio=(1.0, 1.0)),\n","    torchvision.transforms.RandomHorizontalFlip(),\n","    torchvision.transforms.ToTensor(),\n","    # 标准化图像的每个通道\n","    torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],[0.2023, 0.1994, 0.2010]) \n","])\n","\n","test_augs = torchvision.transforms.Compose([\n","     torchvision.transforms.ToTensor(),\n","     torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],[0.2023, 0.1994, 0.2010]) \n","])"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-04-16T14:20:34.058409Z","iopub.status.busy":"2022-04-16T14:20:34.058152Z","iopub.status.idle":"2022-04-16T14:20:34.066065Z","shell.execute_reply":"2022-04-16T14:20:34.065367Z","shell.execute_reply.started":"2022-04-16T14:20:34.058376Z"},"trusted":true},"outputs":[],"source":["def load_cifar10(is_train, augs, batch_size):\n","    dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=is_train,\n","                                           transform=augs, download=True)\n","    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n","                    shuffle=is_train, num_workers=d2l.get_dataloader_workers())\n","    return dataloader"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-04-16T14:20:34.067854Z","iopub.status.busy":"2022-04-16T14:20:34.067539Z","iopub.status.idle":"2022-04-16T14:20:34.085054Z","shell.execute_reply":"2022-04-16T14:20:34.084245Z","shell.execute_reply.started":"2022-04-16T14:20:34.067758Z"},"trusted":true},"outputs":[],"source":["#@save\n","def train_batch_ch13(net, X, y, loss, trainer, devices):\n","    \"\"\"用多GPU进行小批量训练\"\"\"\n","    if isinstance(X, list):\n","        # 微调BERT中所需（稍后讨论）\n","        X = [x.to(devices[0]) for x in X]\n","    else:\n","        X = X.to(devices[0])\n","    y = y.to(devices[0])\n","    net.train()\n","    trainer.zero_grad()\n","    pred = net(X)\n","    l = loss(pred, y)\n","    l.sum().backward()\n","    trainer.step()\n","    train_loss_sum = l.sum()\n","    train_acc_sum = d2l.accuracy(pred, y)\n","    return train_loss_sum, train_acc_sum\n","\n","#@save\n","def train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,devices=d2l.try_all_gpus()):\n","    \"\"\"用多GPU进行模型训练\"\"\"\n","    timer, num_batches = d2l.Timer(), len(train_iter)\n","    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],\n","                            legend=['train loss', 'train acc', 'test acc'])\n","    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n","\n","    scheduler = torch.optim.lr_scheduler.StepLR(trainer, 4, 0.9)#加入调学习率的优化器\n","    \n","    for epoch in range(num_epochs):\n","        # 4个维度：储存训练损失，训练准确度，实例数，特点数\n","        metric = d2l.Accumulator(4)\n","        for i, (features, labels) in enumerate(train_iter):\n","            timer.start()\n","            l, acc = train_batch_ch13(\n","                net, features, labels, loss, trainer, devices)\n","            metric.add(l, acc, labels.shape[0], labels.numel())\n","            timer.stop()\n","            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n","                animator.add(epoch + (i + 1) / num_batches,\n","                             (metric[0] / metric[2], metric[1] / metric[3],\n","                              None))\n","        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n","        animator.add(epoch + 1, (None, None, test_acc))\n","        scheduler.step()\n","    print(f'loss {metric[0] / metric[2]:.3f}, train acc '\n","          f'{metric[1] / metric[3]:.3f}, test acc {test_acc:.3f}')\n","    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '\n","          f'{str(devices)}')"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-04-16T14:20:34.086451Z","iopub.status.busy":"2022-04-16T14:20:34.086100Z","iopub.status.idle":"2022-04-16T14:20:34.096976Z","shell.execute_reply":"2022-04-16T14:20:34.096246Z","shell.execute_reply.started":"2022-04-16T14:20:34.086415Z"},"trusted":true},"outputs":[],"source":["batch_size, devices = 512, d2l.try_all_gpus()\n","\n","\n","def train_with_data_aug(train_augs, test_augs, net, lr=0.06):\n","    train_iter = load_cifar10(True, train_augs, batch_size)\n","    test_iter = load_cifar10(False, test_augs, batch_size)\n","    loss = nn.CrossEntropyLoss(reduction=\"none\")\n","    trainer = torch.optim.Adam(net.parameters(), lr=lr)\n","    train_ch13(net, train_iter, test_iter, loss, trainer, 30, devices)"]},{"cell_type":"markdown","metadata":{},"source":["- pretraining"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-04-16T14:20:34.098779Z","iopub.status.busy":"2022-04-16T14:20:34.098495Z","iopub.status.idle":"2022-04-16T14:32:09.263289Z","shell.execute_reply":"2022-04-16T14:32:09.262262Z","shell.execute_reply.started":"2022-04-16T14:20:34.098743Z"},"trusted":true},"outputs":[],"source":["train_with_data_aug(train_augs, test_augs, net)"]},{"cell_type":"markdown","metadata":{},"source":["![pretrain](ResNet_Plus_Result5.png)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-04-16T14:32:09.267937Z","iopub.status.busy":"2022-04-16T14:32:09.267624Z","iopub.status.idle":"2022-04-16T14:32:09.273681Z","shell.execute_reply":"2022-04-16T14:32:09.272889Z","shell.execute_reply.started":"2022-04-16T14:32:09.267884Z"},"trusted":true},"outputs":[],"source":["print(net)"]},{"cell_type":"markdown","metadata":{},"source":["- add a 1x1 convolutional layel"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-04-16T14:32:09.275749Z","iopub.status.busy":"2022-04-16T14:32:09.274980Z","iopub.status.idle":"2022-04-16T14:32:09.286301Z","shell.execute_reply":"2022-04-16T14:32:09.285332Z","shell.execute_reply.started":"2022-04-16T14:32:09.275709Z"},"trusted":true},"outputs":[],"source":["b0 = nn.Conv2d(1, 3, kernel_size=1)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-04-16T14:32:09.287695Z","iopub.status.busy":"2022-04-16T14:32:09.287490Z","iopub.status.idle":"2022-04-16T14:32:09.297981Z","shell.execute_reply":"2022-04-16T14:32:09.296972Z","shell.execute_reply.started":"2022-04-16T14:32:09.287663Z"},"trusted":true},"outputs":[],"source":["fnet=nn.Sequential(b0,net)\n","print(fnet)"]},{"cell_type":"markdown","metadata":{},"source":["- add a MLP"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-04-16T14:32:09.299934Z","iopub.status.busy":"2022-04-16T14:32:09.299418Z","iopub.status.idle":"2022-04-16T14:32:09.311958Z","shell.execute_reply":"2022-04-16T14:32:09.311166Z","shell.execute_reply.started":"2022-04-16T14:32:09.299878Z"},"trusted":true},"outputs":[],"source":["b1 = nn.Linear(10, 256)\n","b2 = nn.ReLU()\n","b3 = nn.Linear(256,10)\n","fnet.add_module(\"fc1\",b1)\n","fnet.add_module(\"fc2\",b2)\n","fnet.add_module(\"fc3\",b3)\n","nn.init.xavier_uniform_(fnet.fc1.weight)\n","nn.init.xavier_uniform_(fnet.fc3.weight)\n","print(fnet)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-04-16T14:32:09.313324Z","iopub.status.busy":"2022-04-16T14:32:09.313111Z","iopub.status.idle":"2022-04-16T14:32:09.324281Z","shell.execute_reply":"2022-04-16T14:32:09.323427Z","shell.execute_reply.started":"2022-04-16T14:32:09.313296Z"},"trusted":true},"outputs":[],"source":["def evaluate_accuracy_gpu(net, data_iter, device=None): #@save\n","    \"\"\"使用GPU计算模型在数据集上的精度\"\"\"\n","    if isinstance(net, nn.Module):\n","        net.eval()  # 设置为评估模式\n","        if not device:\n","            device = next(iter(net.parameters())).device\n","    # 正确预测的数量，总预测的数量\n","    metric = d2l.Accumulator(2)\n","    with torch.no_grad():\n","        for X, y in data_iter:\n","            if isinstance(X, list):\n","                # BERT微调所需的（之后将介绍）\n","                X = [x.to(device) for x in X]\n","            else:\n","                X = X.to(device)\n","            y = y.to(device)\n","            metric.add(d2l.accuracy(net(X), y), y.numel())\n","    return metric[0] / metric[1]"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-04-16T14:32:09.326149Z","iopub.status.busy":"2022-04-16T14:32:09.325815Z","iopub.status.idle":"2022-04-16T14:32:09.363343Z","shell.execute_reply":"2022-04-16T14:32:09.362695Z","shell.execute_reply.started":"2022-04-16T14:32:09.326046Z"},"trusted":true},"outputs":[],"source":["for name, param in fnet.named_parameters():\n","    print(name)\n","    print(\"-----------------------------------\")"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-04-16T14:32:09.364671Z","iopub.status.busy":"2022-04-16T14:32:09.364424Z","iopub.status.idle":"2022-04-16T14:32:09.371949Z","shell.execute_reply":"2022-04-16T14:32:09.371193Z","shell.execute_reply.started":"2022-04-16T14:32:09.364637Z"},"trusted":true},"outputs":[],"source":["def load_FashionMNIST(is_train, augs, batch_size):\n","    dataset = torchvision.datasets.FashionMNIST(root=\"./data\", train=is_train,\n","                                           transform=augs, download=True)\n","    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n","                    shuffle=is_train, num_workers=d2l.get_dataloader_workers())\n","    \n","    return dataloader"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-04-16T14:32:09.373550Z","iopub.status.busy":"2022-04-16T14:32:09.373240Z","iopub.status.idle":"2022-04-16T14:32:09.382200Z","shell.execute_reply":"2022-04-16T14:32:09.380608Z","shell.execute_reply.started":"2022-04-16T14:32:09.373515Z"},"trusted":true},"outputs":[],"source":["# cifar10 32*32\n","train_augs = torchvision.transforms.Compose([\n","     # 在高度和宽度上将图像放大到40像素的正方形\n","    torchvision.transforms.Resize(40),\n","    # 随机裁剪出一个高度和宽度均为40像素的正方形图像，\n","    # 生成一个面积为原始图像面积0.64到1倍的小正方形，\n","    # 然后将其缩放为高度和宽度均为32像素的正方形\n","    torchvision.transforms.RandomResizedCrop(32, scale=(0.64, 1.0),ratio=(1.0, 1.0)),\n","    torchvision.transforms.ToTensor(),\n","    # 标准化图像的每个通道\n","    torchvision.transforms.Normalize([0.4914],[0.2023]) \n","])\n","\n","test_augs = torchvision.transforms.Compose([\n","     torchvision.transforms.Resize(32),\n","     torchvision.transforms.ToTensor(),\n","     torchvision.transforms.Normalize([0.4914],[0.2023]) \n","])"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2022-04-16T14:32:09.384157Z","iopub.status.busy":"2022-04-16T14:32:09.383390Z","iopub.status.idle":"2022-04-16T14:32:09.396554Z","shell.execute_reply":"2022-04-16T14:32:09.395672Z","shell.execute_reply.started":"2022-04-16T14:32:09.384111Z"},"trusted":true},"outputs":[],"source":["def train_fine_tuning(net, learning_rate, batch_size=256, num_epochs=20,\n","                      param_group=True):\n","    train_iter = load_FashionMNIST(True, train_augs, batch_size)\n","    test_iter = load_FashionMNIST(False, test_augs, batch_size)\n","    devices = d2l.try_all_gpus()\n","    loss = nn.CrossEntropyLoss(reduction=\"none\")\n","    if param_group:\n","        params_1x = [param for name, param in net.named_parameters()if name not in [\n","                             \"fc1.weight\", \"fc1.bias\",\n","                             \"fc3.weight\", \"fc3.bias\",\n","                             \"0.weight\", \"0.bias\"\n","                            ]\n","                    ]\n","        trainer = torch.optim.SGD([{'params': params_1x},\n","                                   {'params': net.fc1.parameters(),'lr': learning_rate * 20},\n","                                   {'params': net.fc3.parameters(),'lr': learning_rate * 10},\n","                                   {'params': net[0].parameters(),'lr': learning_rate * 50}\n","                                  ],lr=learning_rate, weight_decay=0.001)\n","    else:\n","        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate*100,weight_decay=0.001)\n","    d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\n","                   devices)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-04-16T14:41:29.830983Z","iopub.status.busy":"2022-04-16T14:41:29.830384Z","iopub.status.idle":"2022-04-16T14:49:07.046697Z","shell.execute_reply":"2022-04-16T14:49:07.045810Z","shell.execute_reply.started":"2022-04-16T14:41:29.830946Z"},"trusted":true},"outputs":[],"source":["train_fine_tuning(fnet,1e-4)"]},{"cell_type":"markdown","metadata":{},"source":["# Running on Kaggle"]},{"cell_type":"markdown","metadata":{},"source":["- The initial ResNet model"]},{"cell_type":"markdown","metadata":{},"source":["![the first](ResNet_Result1.png)\n","![the second](ResNet_Result2.png)"]},{"cell_type":"markdown","metadata":{},"source":["- 在Cifar10进行预训练，并做微调"]},{"cell_type":"markdown","metadata":{},"source":["![result1](ResNet_Plus_Result1.png)\n","![result2](ResNet_Plus_Result2.png)\n","![result3](ResNet_Plus_Result3.png)\n","![result4](ResNet_Plus_Result4.png)"]},{"cell_type":"markdown","metadata":{},"source":["- 在ResNet中加入了CBAM注意力机制，对Cifar10做了数据增广后进行预训练，训练过程使用了根据epoch数衰减学习率的optim，然后进行微调"]},{"cell_type":"markdown","metadata":{},"source":["![fashion_result](ResNet_Plus_Result6.png)\n","![fashion_result1](ResNet_Plus_Result7.png)"]},{"cell_type":"markdown","metadata":{},"source":["# summary\n","        在进行多次调参和改变网络结构后最后的结果反而更差了，主要原因我认为是改变了预训练模型中前面的结构，所以可以认为模型对图片特征的提取需要多层协调作用，但上面模型中在预训练之后在开头加了一层1x1卷积层，应该是这里一定程度上破坏了预训练的效果。\n","        总的来说，这个ResNet_Plus主要是为了实操前面所学缝合而成的，即尽量手动实现。这些所学包括：构建基本网络，添加SENet、CBAM注意力module、数据增广（Data_Augumentation)、预训练的做法、微调（finetune）、可进行learning rate_decay的optim。虽然这些缝合的方法在这里效果一般，但我后面将其用在了Cifar10分类任务上，达到了95%的test_acc。还是蛮好的\n","        至此，图片分类网络的学习告一段落。"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
